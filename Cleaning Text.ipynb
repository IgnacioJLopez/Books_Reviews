{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca7c7055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string   # libreria de cadena de caracteres\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22801a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/nacho/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/nacho/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/nacho/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/nacho/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Objetos de sklearn para hacer tópicos\n",
    "from sklearn.feature_extraction.text import CountVectorizer # Contador de frecuencia\n",
    "from sklearn.feature_extraction.text import TfidfTransformer # Creador de tf-idf\n",
    "\n",
    "# Normalizador\n",
    "from sklearn.preprocessing import Normalizer \n",
    "\n",
    "# Algoritmos de descomposición de tópicos\n",
    "from sklearn.decomposition import NMF \n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Importamos nltk para extraer stopwords \n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Librería para hacer wordclouds\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49fbcfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos el dataframe de entrenamiento y el dataframe de testeo\n",
    "filename_train = '/home/nacho/Documentos/Python/BooksReviews/goodreads_train.csv' \n",
    "filename_test = '/home/nacho/Documentos/Python/BooksReviews/goodreads_test.csv'\n",
    "\n",
    "df_train = pd.read_csv(filename_train)\n",
    "df_test = pd.read_csv(filename_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb947232",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=df_test.drop_duplicates()\n",
    "\n",
    "df_train=df_train.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86cca675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "869012"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sincero = df_train[df_train['rating']!=0].copy().reset_index(drop = True)\n",
    "len(df_sincero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51ce90b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23de87a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # pasa las mayusculas del texto a minusculas\n",
    "    text = text.lower()\n",
    "    #Eliminar citas\n",
    "    text = re.sub(r'\"[^\"]*\"','', text)\n",
    "    # reemplaza texto entre corchetes por espacio en blanco.. ¿ y \\% no se..\n",
    "    text = re.sub('\\[.*?¿\\]\\%', ' ', text)                           \n",
    "    # reemplaza signos de puntuacion por espacio en blanco.. %s -> \\S+ es cualquier caracter que no sea un espacio en blanco\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text) \n",
    "    # remueve palabras que contienen numeros.\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    \n",
    "    # Sacamos comillas, los puntos suspensivos, <<, >>\n",
    "    text = re.sub('[‘’“”…«»]', '', text)\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dfa8225",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_sincero.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47057cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "round0 = lambda x: clean_text(x)\n",
    "\n",
    "# Dataframe que resulta de aplicarle a las columnas la funcion de limpieza\n",
    "review_text = pd.DataFrame(data.review_text.apply(round0))\n",
    "\n",
    "data.drop('review_text', axis=1, inplace=True)\n",
    "\n",
    "data['review_text'] = review_text\n",
    "\n",
    "del(review_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54d05db",
   "metadata": {},
   "source": [
    "NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0d8a991",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() # funcion para lematizar\n",
    "sw = nltk.corpus.stopwords.words('english') # lista de stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e40811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sw(text):\n",
    "    words = text.split(' ') # separamos por espacios\n",
    "    words_clean = []\n",
    "    for w in words:\n",
    "        if not w in sw: # si no es stopword, agregamos la version lematizada\n",
    "            words_clean.append(lemmatizer.lemmatize(w))\n",
    "    return ' '.join(words_clean)\n",
    "\n",
    "round3 = lambda x: remove_sw(x)\n",
    " \n",
    "review_text =  pd.DataFrame(data.review_text.apply(round3))\n",
    "\n",
    "data.drop('review_text', axis=1, inplace=True)\n",
    "\n",
    "data['review_text'] = review_text\n",
    "\n",
    "del(review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5b90e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()\n",
    "\n",
    "# Lista de stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Creamos el objeto contador de palabras, pidiéndole que remueve\n",
    "# las stopwords, los términos que aparecen en un único documento (min_df)\n",
    "# y los términos que aparecen en más del 70% de los documentos (max_df).\n",
    "# Esto es para eliminar palabras raras (o errores de tipeo) y \n",
    "# términos que seguramente son stopwords no incluídos en la lista\n",
    "count = CountVectorizer(min_df = 2, max_df = 0.70, stop_words = stopwords)\n",
    "\n",
    "# Ajustamos con los datos. Acá especificamente creamos una matriz documentos-términos\n",
    "x_count = count.fit_transform(df['review_text'])\n",
    "\n",
    "# Dimensions de la matriz doc-tér\n",
    "print(x_count.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d9855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el objeto tf-idf. Le decimos además que devuelva los\n",
    "# vectores documento con norma euclídea igual a 1 (norm = 'l2')\n",
    "tfidf = TfidfTransformer(norm = 'l2')\n",
    "\n",
    "# Creamos la matriz tf-idf a partir de la matriz de frecuencias\n",
    "x_tfidf = tfidf.fit_transform(x_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa002a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elijamos la cantidad de tópicos\n",
    "n_components = 5\n",
    "\n",
    "# Construímos el objeto NMF con los tópicos indicados \n",
    "nmf = NMF(n_components = n_components)\n",
    "\n",
    "# Aplicamos sobre nuestros datos\n",
    "x_nmf = nmf.fit_transform(x_tfidf)\n",
    "\n",
    "# Dimensión de la matriz transformada\n",
    "\n",
    "print(x_nmf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4d435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objeto índice: término de nuestro vocabulario\n",
    "vocabulary = {item: key for key, item in count.vocabulary_.items()}\n",
    "\n",
    "# Para cada componente\n",
    "for n in range(n_components):\n",
    "\n",
    "    # Ordenamos una lista del largo de nuestro vocabulario según el peso en cada componente y nos quedamos con los primeros 10\n",
    "    list_sorted = sorted(range(nmf.components_.shape[1]), reverse = True, key = lambda x: nmf.components_[n][x])[:10]\n",
    "\n",
    "    # Printeamos los términos asociados a los valores más grande de cada una de las componentes\n",
    "    print(', '.join([vocabulary[i] for i in list_sorted]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43bd7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "# WordClouds\n",
    "wc_atributos = {'height' : 800,\n",
    "                'width' : 1200,\n",
    "                'background_color' : 'white',\n",
    "                'max_words' : 20\n",
    "                } # Defino los parámetros que les voy a pasar a los wordclouds\n",
    "\n",
    "# Creo la figura\n",
    "fig, axs = plt.subplots(n_components, figsize = (6,20))\n",
    "\n",
    "# Recorro para todas las componentes\n",
    "for n in range(n_components):\n",
    "\n",
    "    # 10 términos más pesados\n",
    "    list_sorted = sorted(range(len(vocabulary)), reverse = True, key = lambda x: nmf.components_[n][x])[:10]\n",
    "\n",
    "    # Diccionario término: peso\n",
    "    comp_dict = {vocabulary[i]: nmf.components_[n][i] for i in list_sorted}\n",
    "\n",
    "    # Creo el wordlcoud\n",
    "    wc = WordCloud(**wc_atributos # De esta forma, le estoy diciendo a la función que expanda el diccionario de atributos de forma tal de que entienda lo que quiero que haga\n",
    "                 ).generate_from_frequencies(comp_dict)\n",
    "\n",
    "    axs[n].set_title('Tópico {}'.format(n))\n",
    "    axs[n].imshow(wc)\n",
    "    axs[n].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acad008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un objeto para normalizar a que la suma dé 1\n",
    "norm = Normalizer('l1')\n",
    "\n",
    "# Sobreescribimos sobre la matriz de documentos-tópicos\n",
    "x_nmf = norm.fit_transform(x_nmf)\n",
    "\n",
    "# Guardemos en el dataframe esta información\n",
    "for n in range(n_components):\n",
    "    df['nmf_comp{}'.format(n)] = x_nmf[:,n]\n",
    "\n",
    "df_rating = df.groupby('rating').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d549fb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "# El eje x es la década\n",
    "x = df_rating.index\n",
    "\n",
    "# El eje y son las distribuciones\n",
    "y = df_rating[['nmf_comp{}'.format(i) for i in range(n_components)]].to_numpy()\n",
    "\n",
    "plt.figure(figsize = (8,5))\n",
    "plt.stackplot(x, y.T) # Stackplot: sirve para graficar distribuciones\n",
    "plt.xlim([0, 5])\n",
    "plt.ylim([0, 1.00])\n",
    "plt.yticks([])\n",
    "plt.xlabel('Rating')\n",
    "plt.legend(['Tópico {}'.format(i) for i in range(n_components)], loc = (1.05, 0.60))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
